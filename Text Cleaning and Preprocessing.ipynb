{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d77f8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import emoji\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f35cb6b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\doraz\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\doraz\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\doraz\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\doraz\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import ssl\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f93f54a",
   "metadata": {},
   "source": [
    "## emoji related functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0dfceee5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ğŸ¥°'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def emojiname2icon(name):\n",
    "    return emoji.emojize(f\":{name.replace(' ','_')}:\")\n",
    "\n",
    "emojiname2icon('smiling face with hearts')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba52def6",
   "metadata": {},
   "source": [
    "## Function: Remove special characters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd3018cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Production appear product door '"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def remove_special(text):\n",
    "    # Remove URLs\n",
    "    url_pattern = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    text = url_pattern.sub('', text)\n",
    "    \n",
    "    # Remove words starting with # and @ for all languages\n",
    "    hashtag_pattern = re.compile(r'(?:^|\\s)[@#](?:\\w+|[^\\s\\w])+')\n",
    "    text = hashtag_pattern.sub('', text)\n",
    "    \n",
    "    # Remove punctuation\n",
    "    punctuation_pattern = re.compile(r'[^\\w\\s]', re.UNICODE)\n",
    "    text = punctuation_pattern.sub('', text)\n",
    "    \n",
    "\n",
    "    # Remove emoji, number, tab\n",
    "    emoji_number_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"0-9\"           # numbers\n",
    "        u\"\\n\\t\"           # newline and tab characters\n",
    "\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    text = emoji_number_pattern.sub(r'', text)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    return text\n",
    "\n",
    "remove_special('ğŸ’‹ğŸ’«ğŸ˜€\\nProduction appear product door 1234')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed9312e",
   "metadata": {},
   "source": [
    "## Function: Remove Stop words and lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b2a4e26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\doraz\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "  \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def remove_stopword_and_lemmatized(text):\n",
    "\n",
    "  \n",
    "  #tokenized and change to lower\n",
    "    word_tokens = word_tokenize(text.lower())\n",
    "\n",
    "  #remove stop word\n",
    "    filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
    "\n",
    "\n",
    "  #get part of speech\n",
    "    pos = nltk.pos_tag(filtered_sentence)\n",
    "\n",
    "\n",
    "    #lemmatization with POS\n",
    "    lemmeanized_sentence = [lemmatizer.lemmatize(filtered_sentence[i],get_wordnet_pos(pos[i][1])) if get_wordnet_pos(pos[i][1]) != None else lemmatizer.lemmatize(filtered_sentence[i]) for i in range(len(filtered_sentence))]\n",
    "\n",
    "      #remove duplicates (in order)\n",
    "    return \" \".join(list(dict.fromkeys(lemmeanized_sentence)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6648b239",
   "metadata": {},
   "source": [
    "## Function: apply preprocessing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b0c14fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def content_preprocessing(df):\n",
    "    df['cleaned content'] = df['content'].apply(remove_special)\n",
    "    df['cleaned_content'] = df['cleaned content'].apply(remove_stopword_and_lemmatized)\n",
    "    \n",
    "    #get emoji id for name\n",
    "    df['emoji_id'] = df['emoji_name'].apply(lambda name: emojiname2id_dic[name])\n",
    "    \n",
    "    return df[['emoji_id','content','cleaned_content']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5cde62",
   "metadata": {},
   "source": [
    "## Combine data from different data sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8172187e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Produce Emoji_DF[emoji_id, emoji_name]\n",
    "top_20_emoji_name = [\n",
    "\n",
    "    'face with tears of joy', # funny happy\n",
    "    \n",
    "    'red heart', #love\n",
    "    \n",
    "    'broken heart', #no love\n",
    "    \n",
    "    'thumbs up', #encouragement\n",
    "    \n",
    "    'smiling face with smiling eyes', #happy\n",
    "    \n",
    "    'loudly crying face', #pure sad\n",
    "    \n",
    "    'clapping hands', #congradualation\n",
    "    \n",
    "    'fire', #hot /sexy\n",
    "    \n",
    "    'face screaming in fear', #shock\n",
    "    \n",
    "    'pile of poo', #non-sense, disapproval\n",
    "    \n",
    "    'face with symbols on mouth', #anger\n",
    "    \n",
    "    'eggplant', #horny\n",
    "    \n",
    "    'face savoring food', # craving\n",
    "    \n",
    "    'hundred points', #approval\n",
    "    \n",
    "    'folded hands', #pray\n",
    "    \n",
    "    \n",
    "]\n",
    "\n",
    "emoji_reference = {'emoji_name':top_20_emoji_name,'emoji_id':[i for i in range(len(top_20_emoji_name))]}\n",
    "emoji_df =  pd.DataFrame(emoji_reference)\n",
    "emoji_df\n",
    "\n",
    "emojiname2id_dic = {emoji_df.emoji_name[i]:emoji_df.index[i] for i in range(len(emoji_df.index))}\n",
    "emojiid2name_dic = {emoji_df.index[i]:emoji_df.emoji_name[i] for i in range(len(emoji_df.index))}\n",
    "\n",
    "\n",
    "# load data from different data source\n",
    "scrape1 = pd.read_csv('processed_data.csv', index_col = 0)\n",
    "scrape2 = pd.read_csv('processed_data_batch2.csv')\n",
    "\n",
    "\n",
    "df = pd.DataFrame(columns=['emoji_name','emoji','content','cleaned content'])\n",
    "df = pd.concat([df,scrape1,scrape2])\n",
    "top_20_emoji_df = pd.DataFrame(columns=['emoji_name','emoji','content','cleaned content'])\n",
    "for name in top_20_emoji_name:\n",
    "    top_20_emoji_df = pd.concat([top_20_emoji_df,df[df.emoji_name==name]])\n",
    "\n",
    "kaggle_tweets = pd.read_csv('kaggle_tweets 15 selected emoji.csv',names=['emoji_name','content'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d02b3cd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7390718, 2)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kaggle_tweets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "609feb9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emoji_name</th>\n",
       "      <th>emoji</th>\n",
       "      <th>content</th>\n",
       "      <th>cleaned content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>face with tears of joy</td>\n",
       "      <td>ğŸ˜‚</td>\n",
       "      <td>@PeakSanti But why this photo?ğŸ˜€ğŸ˜€ğŸ˜‚ğŸ˜‚</td>\n",
       "      <td>But why this photo?ğŸ˜€ğŸ˜€ğŸ˜‚ğŸ˜‚</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>face with tears of joy</td>\n",
       "      <td>ğŸ˜‚</td>\n",
       "      <td>@Sachin_Ro45 Mass troll ğŸ˜€ğŸ˜€ğŸ˜€ğŸ˜€ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚\\n@AdnanSami...</td>\n",
       "      <td>Mass troll ğŸ˜€ğŸ˜€ğŸ˜€ğŸ˜€ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚\\n plz watch this video</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>386</th>\n",
       "      <td>face with tears of joy</td>\n",
       "      <td>ğŸ˜‚</td>\n",
       "      <td>The people I work with thinks the bank has all...</td>\n",
       "      <td>The people I work with thinks the bank has all...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>426</th>\n",
       "      <td>face with tears of joy</td>\n",
       "      <td>ğŸ˜‚</td>\n",
       "      <td>@Jamespa97888596 ğŸ˜‚ğŸ˜‚ğŸ˜‚ all friends again ğŸ‘ŒğŸ‘ŒğŸ˜€</td>\n",
       "      <td>ğŸ˜‚ğŸ˜‚ğŸ˜‚ all friends again ğŸ‘ŒğŸ‘ŒğŸ˜€</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>454</th>\n",
       "      <td>face with tears of joy</td>\n",
       "      <td>ğŸ˜‚</td>\n",
       "      <td>How you so in love with a nigga that's never w...</td>\n",
       "      <td>How you so in love with a nigga that's never w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7390713</th>\n",
       "      <td>loudly crying face</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&amp;amp; to think I was going to split my tax ref...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7390714</th>\n",
       "      <td>face with tears of joy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&amp;amp; to think I was going to split my tax ref...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7390715</th>\n",
       "      <td>loudly crying face</td>\n",
       "      <td>NaN</td>\n",
       "      <td>please !! remove the suspension from the pleas...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7390716</th>\n",
       "      <td>clapping hands</td>\n",
       "      <td>NaN</td>\n",
       "      <td>please. ğŸ‘ğŸ¼ so because i stan someone means i d...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7390717</th>\n",
       "      <td>face with tears of joy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>please. ğŸ‘ğŸ¼ so because i stan someone means i d...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7492030 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     emoji_name emoji  \\\n",
       "47       face with tears of joy     ğŸ˜‚   \n",
       "206      face with tears of joy     ğŸ˜‚   \n",
       "386      face with tears of joy     ğŸ˜‚   \n",
       "426      face with tears of joy     ğŸ˜‚   \n",
       "454      face with tears of joy     ğŸ˜‚   \n",
       "...                         ...   ...   \n",
       "7390713      loudly crying face   NaN   \n",
       "7390714  face with tears of joy   NaN   \n",
       "7390715      loudly crying face   NaN   \n",
       "7390716          clapping hands   NaN   \n",
       "7390717  face with tears of joy   NaN   \n",
       "\n",
       "                                                   content  \\\n",
       "47                      @PeakSanti But why this photo?ğŸ˜€ğŸ˜€ğŸ˜‚ğŸ˜‚   \n",
       "206      @Sachin_Ro45 Mass troll ğŸ˜€ğŸ˜€ğŸ˜€ğŸ˜€ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚\\n@AdnanSami...   \n",
       "386      The people I work with thinks the bank has all...   \n",
       "426             @Jamespa97888596 ğŸ˜‚ğŸ˜‚ğŸ˜‚ all friends again ğŸ‘ŒğŸ‘ŒğŸ˜€   \n",
       "454      How you so in love with a nigga that's never w...   \n",
       "...                                                    ...   \n",
       "7390713  &amp; to think I was going to split my tax ref...   \n",
       "7390714  &amp; to think I was going to split my tax ref...   \n",
       "7390715  please !! remove the suspension from the pleas...   \n",
       "7390716  please. ğŸ‘ğŸ¼ so because i stan someone means i d...   \n",
       "7390717  please. ğŸ‘ğŸ¼ so because i stan someone means i d...   \n",
       "\n",
       "                                           cleaned content  \n",
       "47                                 But why this photo?ğŸ˜€ğŸ˜€ğŸ˜‚ğŸ˜‚  \n",
       "206           Mass troll ğŸ˜€ğŸ˜€ğŸ˜€ğŸ˜€ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚\\n plz watch this video  \n",
       "386      The people I work with thinks the bank has all...  \n",
       "426                              ğŸ˜‚ğŸ˜‚ğŸ˜‚ all friends again ğŸ‘ŒğŸ‘ŒğŸ˜€  \n",
       "454      How you so in love with a nigga that's never w...  \n",
       "...                                                    ...  \n",
       "7390713                                                NaN  \n",
       "7390714                                                NaN  \n",
       "7390715                                                NaN  \n",
       "7390716                                                NaN  \n",
       "7390717                                                NaN  \n",
       "\n",
       "[7492030 rows x 4 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#concat everything togehter \n",
    "total_df = pd.concat([top_20_emoji_df,kaggle_tweets])\n",
    "total_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273d45ed",
   "metadata": {},
   "source": [
    "## sampled training and testing data from combined data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "79b79e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n_sample(df, n, random_state):\n",
    "    sample_data =  pd.DataFrame()\n",
    "    for name in top_20_emoji_name:\n",
    "        sample = df[df.emoji_name == name].sample(n=n,  random_state=random_state)\n",
    "        sample_data = pd.concat([sample_data,sample])\n",
    "    return sample_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "54b32ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#randomly sampled from total df for tarining and testing \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Change sample size to 10000 from 8000; test_size changed to 0.2 from 0,4\n",
    "a = get_n_sample(total_df,15000,4222)\n",
    "train, test = train_test_split(get_n_sample(total_df,15000,4222), test_size=0.3,random_state=4222)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8e17f80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply data preprocessing on selected content\n",
    "train_df, test_df = content_preprocessing(train), content_preprocessing(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e21e179e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wh/fjfmn_7x52l_pzdjc46xflpr0000gn/T/ipykernel_19053/1965433990.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_df['cleaned_content_len']= train_df.cleaned_content.apply(lambda x: len(x.split()))\n",
      "/var/folders/wh/fjfmn_7x52l_pzdjc46xflpr0000gn/T/ipykernel_19053/1965433990.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df['cleaned_content_len']= test_df.cleaned_content.apply(lambda x: len(x.split()))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emoji_id</th>\n",
       "      <th>content</th>\n",
       "      <th>cleaned_content</th>\n",
       "      <th>cleaned_content_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>219336</th>\n",
       "      <td>7</td>\n",
       "      <td>announcement on Sunday 6pm ! ğŸ”¥\\n</td>\n",
       "      <td>announcement sunday pm</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000022</th>\n",
       "      <td>6</td>\n",
       "      <td>Chelsea legend Frank Lampard will receive the ...</td>\n",
       "      <td>chelsea legend frank lampard receive football ...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4886330</th>\n",
       "      <td>8</td>\n",
       "      <td>HE JUST DESTROYED THIS MAN ğŸ˜±\\n</td>\n",
       "      <td>destroy man</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57132</th>\n",
       "      <td>10</td>\n",
       "      <td>Itâ€™s twitter Iâ€™m sure ğŸ¤¬\\n</td>\n",
       "      <td>twitter im sure</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2479832</th>\n",
       "      <td>5</td>\n",
       "      <td>Yeah and I think u need a phone no tooğŸ˜­\\n</td>\n",
       "      <td>yeah think u need phone</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2678399</th>\n",
       "      <td>6</td>\n",
       "      <td>Dirty Heads - Celebrate feat. The Unlikely Can...</td>\n",
       "      <td>dirty head celebrate feat unlikely candidate o...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3440404</th>\n",
       "      <td>4</td>\n",
       "      <td>ETHAN AND GRAYSON WERE NOMINATED FOR A SHORTY ...</td>\n",
       "      <td>ethan grayson nominate shorty award make sure ...</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1005035</th>\n",
       "      <td>1</td>\n",
       "      <td>[EXO-CBX ALBUM GIVEAWAY] Hi! will be hosting h...</td>\n",
       "      <td>exocbx album giveaway hi host first please giv...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3623033</th>\n",
       "      <td>13</td>\n",
       "      <td>TEN who? Heâ€™s â€œHUNDREDâ€ now!! ğŸ˜‚ğŸ’¯ğŸ™ŒğŸ¼\\n</td>\n",
       "      <td>ten he hundred</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5331016</th>\n",
       "      <td>12</td>\n",
       "      <td>High school senior vs. College senior Same per...</td>\n",
       "      <td>high school senior v college person less angry...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>156729 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         emoji_id                                            content  \\\n",
       "219336          7                   announcement on Sunday 6pm ! ğŸ”¥\\n   \n",
       "2000022         6  Chelsea legend Frank Lampard will receive the ...   \n",
       "4886330         8                     HE JUST DESTROYED THIS MAN ğŸ˜±\\n   \n",
       "57132          10                          Itâ€™s twitter Iâ€™m sure ğŸ¤¬\\n   \n",
       "2479832         5          Yeah and I think u need a phone no tooğŸ˜­\\n   \n",
       "...           ...                                                ...   \n",
       "2678399         6  Dirty Heads - Celebrate feat. The Unlikely Can...   \n",
       "3440404         4  ETHAN AND GRAYSON WERE NOMINATED FOR A SHORTY ...   \n",
       "1005035         1  [EXO-CBX ALBUM GIVEAWAY] Hi! will be hosting h...   \n",
       "3623033        13               TEN who? Heâ€™s â€œHUNDREDâ€ now!! ğŸ˜‚ğŸ’¯ğŸ™ŒğŸ¼\\n   \n",
       "5331016        12  High school senior vs. College senior Same per...   \n",
       "\n",
       "                                           cleaned_content  \\\n",
       "219336                              announcement sunday pm   \n",
       "2000022  chelsea legend frank lampard receive football ...   \n",
       "4886330                                        destroy man   \n",
       "57132                                      twitter im sure   \n",
       "2479832                            yeah think u need phone   \n",
       "...                                                    ...   \n",
       "2678399  dirty head celebrate feat unlikely candidate o...   \n",
       "3440404  ethan grayson nominate shorty award make sure ...   \n",
       "1005035  exocbx album giveaway hi host first please giv...   \n",
       "3623033                                     ten he hundred   \n",
       "5331016  high school senior v college person less angry...   \n",
       "\n",
       "         cleaned_content_len  \n",
       "219336                     3  \n",
       "2000022                   12  \n",
       "4886330                    2  \n",
       "57132                      3  \n",
       "2479832                    5  \n",
       "...                      ...  \n",
       "2678399                   12  \n",
       "3440404                   13  \n",
       "1005035                   11  \n",
       "3623033                    3  \n",
       "5331016                    9  \n",
       "\n",
       "[156729 rows x 4 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove trained data with cleaned content > 0 words\n",
    "train_df['cleaned_content_len']= train_df.cleaned_content.apply(lambda x: len(x.split()))\n",
    "test_df['cleaned_content_len']= test_df.cleaned_content.apply(lambda x: len(x.split()))\n",
    "train_df, test_df  = train_df[train_df.cleaned_content_len >=1],test_df[test_df.cleaned_content_len >=1]\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "09512e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv('train 15 emoji.csv', index = False)\n",
    "test_df.to_csv('test 15 emoji.csv', index = False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c552bea",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "794bda9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emoji_id</th>\n",
       "      <th>content</th>\n",
       "      <th>cleaned_content</th>\n",
       "      <th>cleaned_content_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>219336</th>\n",
       "      <td>7</td>\n",
       "      <td>announcement on Sunday 6pm ! ğŸ”¥\\n</td>\n",
       "      <td>announcement sunday pm</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000022</th>\n",
       "      <td>6</td>\n",
       "      <td>Chelsea legend Frank Lampard will receive the ...</td>\n",
       "      <td>chelsea legend frank lampard receive football ...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4886330</th>\n",
       "      <td>8</td>\n",
       "      <td>HE JUST DESTROYED THIS MAN ğŸ˜±\\n</td>\n",
       "      <td>destroy man</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57132</th>\n",
       "      <td>10</td>\n",
       "      <td>Itâ€™s twitter Iâ€™m sure ğŸ¤¬\\n</td>\n",
       "      <td>twitter im sure</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2479832</th>\n",
       "      <td>5</td>\n",
       "      <td>Yeah and I think u need a phone no tooğŸ˜­\\n</td>\n",
       "      <td>yeah think u need phone</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2678399</th>\n",
       "      <td>6</td>\n",
       "      <td>Dirty Heads - Celebrate feat. The Unlikely Can...</td>\n",
       "      <td>dirty head celebrate feat unlikely candidate o...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3440404</th>\n",
       "      <td>4</td>\n",
       "      <td>ETHAN AND GRAYSON WERE NOMINATED FOR A SHORTY ...</td>\n",
       "      <td>ethan grayson nominate shorty award make sure ...</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1005035</th>\n",
       "      <td>1</td>\n",
       "      <td>[EXO-CBX ALBUM GIVEAWAY] Hi! will be hosting h...</td>\n",
       "      <td>exocbx album giveaway hi host first please giv...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3623033</th>\n",
       "      <td>13</td>\n",
       "      <td>TEN who? Heâ€™s â€œHUNDREDâ€ now!! ğŸ˜‚ğŸ’¯ğŸ™ŒğŸ¼\\n</td>\n",
       "      <td>ten he hundred</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5331016</th>\n",
       "      <td>12</td>\n",
       "      <td>High school senior vs. College senior Same per...</td>\n",
       "      <td>high school senior v college person less angry...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>156729 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         emoji_id                                            content  \\\n",
       "219336          7                   announcement on Sunday 6pm ! ğŸ”¥\\n   \n",
       "2000022         6  Chelsea legend Frank Lampard will receive the ...   \n",
       "4886330         8                     HE JUST DESTROYED THIS MAN ğŸ˜±\\n   \n",
       "57132          10                          Itâ€™s twitter Iâ€™m sure ğŸ¤¬\\n   \n",
       "2479832         5          Yeah and I think u need a phone no tooğŸ˜­\\n   \n",
       "...           ...                                                ...   \n",
       "2678399         6  Dirty Heads - Celebrate feat. The Unlikely Can...   \n",
       "3440404         4  ETHAN AND GRAYSON WERE NOMINATED FOR A SHORTY ...   \n",
       "1005035         1  [EXO-CBX ALBUM GIVEAWAY] Hi! will be hosting h...   \n",
       "3623033        13               TEN who? Heâ€™s â€œHUNDREDâ€ now!! ğŸ˜‚ğŸ’¯ğŸ™ŒğŸ¼\\n   \n",
       "5331016        12  High school senior vs. College senior Same per...   \n",
       "\n",
       "                                           cleaned_content  \\\n",
       "219336                              announcement sunday pm   \n",
       "2000022  chelsea legend frank lampard receive football ...   \n",
       "4886330                                        destroy man   \n",
       "57132                                      twitter im sure   \n",
       "2479832                            yeah think u need phone   \n",
       "...                                                    ...   \n",
       "2678399  dirty head celebrate feat unlikely candidate o...   \n",
       "3440404  ethan grayson nominate shorty award make sure ...   \n",
       "1005035  exocbx album giveaway hi host first please giv...   \n",
       "3623033                                     ten he hundred   \n",
       "5331016  high school senior v college person less angry...   \n",
       "\n",
       "         cleaned_content_len  \n",
       "219336                     3  \n",
       "2000022                   12  \n",
       "4886330                    2  \n",
       "57132                      3  \n",
       "2479832                    5  \n",
       "...                      ...  \n",
       "2678399                   12  \n",
       "3440404                   13  \n",
       "1005035                   11  \n",
       "3623033                    3  \n",
       "5331016                    9  \n",
       "\n",
       "[156729 rows x 4 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "86bb79ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emoji_id</th>\n",
       "      <th>content</th>\n",
       "      <th>cleaned_content</th>\n",
       "      <th>cleaned_content_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5026717</th>\n",
       "      <td>4</td>\n",
       "      <td>might be reuniting with la famiglia next month...</td>\n",
       "      <td>might reunite la famiglia next month che bello</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5686547</th>\n",
       "      <td>12</td>\n",
       "      <td>getting that booty ate ğŸ‘ğŸ˜‹\\n</td>\n",
       "      <td>get booty ate</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1801897</th>\n",
       "      <td>1</td>\n",
       "      <td>Which baddies wnat to be posted?ğŸ˜‹â¤ï¸\\n</td>\n",
       "      <td>baddie wnat post</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3639073</th>\n",
       "      <td>3</td>\n",
       "      <td>Yeah our King is avid reader and so wise and H...</td>\n",
       "      <td>yeah king avid reader wise son mohammed learns...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381513</th>\n",
       "      <td>7</td>\n",
       "      <td>The HOTTEST Air Jordans Of All TimeğŸ˜ğŸ”¥ğŸ”¥ğŸ”¥ğŸ’¦ğŸ’¦ğŸ‘ŒğŸ»\\n</td>\n",
       "      <td>hot air jordan time</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3775628</th>\n",
       "      <td>1</td>\n",
       "      <td>Veggie Facts! We RULE! â¤ï¸â¤ï¸â¤ï¸\\n</td>\n",
       "      <td>veggie fact rule</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2145057</th>\n",
       "      <td>2</td>\n",
       "      <td>BLESSING ğŸ’‹ğŸ’˜â¤ï¸ğŸ’“ğŸ’”ğŸ’•ğŸ’–ğŸ’—ğŸ’™ğŸ’šğŸ’›ğŸ’œğŸ–¤ğŸ’ğŸ’ğŸ’Ÿâ£ï¸ğŸ’ŸğŸ’ğŸ’ğŸ–¤ğŸ’œğŸ’›ğŸ’šğŸ’™ğŸ’—ğŸ’–ğŸ’•ğŸ’”ğŸ’“â¤ï¸ğŸ’˜ğŸ’‹ğŸ’˜...</td>\n",
       "      <td>blessing</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5485976</th>\n",
       "      <td>5</td>\n",
       "      <td>Ugh this hurts me ğŸ˜­ğŸ˜­ğŸ˜­\\n</td>\n",
       "      <td>ugh hurt</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6916006</th>\n",
       "      <td>0</td>\n",
       "      <td>Perfect example of, \"Tit for tat\":  ğŸ˜‚\\n</td>\n",
       "      <td>perfect example tit tat</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5247363</th>\n",
       "      <td>9</td>\n",
       "      <td>ğŸš¨ğŸš¨Judiciary Cmte Subpoenas ğŸ€FBI for all ğŸ‘‰ğŸ’©Clin...</td>\n",
       "      <td>judiciary cmte subpoena fbi clinton investigat...</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>67131 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         emoji_id                                            content  \\\n",
       "5026717         4  might be reuniting with la famiglia next month...   \n",
       "5686547        12                        getting that booty ate ğŸ‘ğŸ˜‹\\n   \n",
       "1801897         1              Which baddies wnat to be posted?ğŸ˜‹â¤ï¸\\n   \n",
       "3639073         3  Yeah our King is avid reader and so wise and H...   \n",
       "381513          7      The HOTTEST Air Jordans Of All TimeğŸ˜ğŸ”¥ğŸ”¥ğŸ”¥ğŸ’¦ğŸ’¦ğŸ‘ŒğŸ»\\n   \n",
       "...           ...                                                ...   \n",
       "3775628         1                    Veggie Facts! We RULE! â¤ï¸â¤ï¸â¤ï¸\\n   \n",
       "2145057         2  BLESSING ğŸ’‹ğŸ’˜â¤ï¸ğŸ’“ğŸ’”ğŸ’•ğŸ’–ğŸ’—ğŸ’™ğŸ’šğŸ’›ğŸ’œğŸ–¤ğŸ’ğŸ’ğŸ’Ÿâ£ï¸ğŸ’ŸğŸ’ğŸ’ğŸ–¤ğŸ’œğŸ’›ğŸ’šğŸ’™ğŸ’—ğŸ’–ğŸ’•ğŸ’”ğŸ’“â¤ï¸ğŸ’˜ğŸ’‹ğŸ’˜...   \n",
       "5485976         5                            Ugh this hurts me ğŸ˜­ğŸ˜­ğŸ˜­\\n   \n",
       "6916006         0            Perfect example of, \"Tit for tat\":  ğŸ˜‚\\n   \n",
       "5247363         9  ğŸš¨ğŸš¨Judiciary Cmte Subpoenas ğŸ€FBI for all ğŸ‘‰ğŸ’©Clin...   \n",
       "\n",
       "                                           cleaned_content  \\\n",
       "5026717     might reunite la famiglia next month che bello   \n",
       "5686547                                      get booty ate   \n",
       "1801897                                   baddie wnat post   \n",
       "3639073  yeah king avid reader wise son mohammed learns...   \n",
       "381513                                 hot air jordan time   \n",
       "...                                                    ...   \n",
       "3775628                                   veggie fact rule   \n",
       "2145057                                           blessing   \n",
       "5485976                                           ugh hurt   \n",
       "6916006                            perfect example tit tat   \n",
       "5247363  judiciary cmte subpoena fbi clinton investigat...   \n",
       "\n",
       "         cleaned_content_len  \n",
       "5026717                    8  \n",
       "5686547                    3  \n",
       "1801897                    3  \n",
       "3639073                    9  \n",
       "381513                     4  \n",
       "...                      ...  \n",
       "3775628                    3  \n",
       "2145057                    1  \n",
       "5485976                    2  \n",
       "6916006                    4  \n",
       "5247363                   14  \n",
       "\n",
       "[67131 rows x 4 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d6278d38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emoji_name</th>\n",
       "      <th>emoji_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>face with tears of joy</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>red heart</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>broken heart</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>thumbs up</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>smiling face with smiling eyes</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>loudly crying face</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>clapping hands</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>fire</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>face screaming in fear</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>pile of poo</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>face with symbols on mouth</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>eggplant</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>face savoring food</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>hundred points</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>folded hands</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        emoji_name  emoji_id\n",
       "0           face with tears of joy         0\n",
       "1                        red heart         1\n",
       "2                     broken heart         2\n",
       "3                        thumbs up         3\n",
       "4   smiling face with smiling eyes         4\n",
       "5               loudly crying face         5\n",
       "6                   clapping hands         6\n",
       "7                             fire         7\n",
       "8           face screaming in fear         8\n",
       "9                      pile of poo         9\n",
       "10      face with symbols on mouth        10\n",
       "11                        eggplant        11\n",
       "12              face savoring food        12\n",
       "13                  hundred points        13\n",
       "14                    folded hands        14"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emoji_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
